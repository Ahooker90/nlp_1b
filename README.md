# How to run code

## 1. Create Conda Environment:

**conda env create -f environment.yml**

## 2. Activate Conda Environment:

**conda activate nlp_311**

## 3. Create Env File:

**touch .env** (feel free to use any text editor you are comfortable with) \
**vim .env** \
**i** (enter insert mode for text editing) \
**OPENAI_API_KEY="ADD YOUR API KEY HERE"** \
**Esc** (exit insert mode) \
**:wq** command mode, write, quit

## 4. Clone Project Repo:

**git clone https://github.com/Ahooker90/NLP.git**

## 5. Change Directory Into Assignment:

**cd Homework_1_b**

## 6. Run Application:

**chainlit run app.py -w** 

## TODO

<<<<<<< HEAD
1.) Clean assignment description \
2.) Each time a setting is updated the VDB is reloaded. This creates excessive wait times. \
3.) The app should not start until the VDB is loaded. Currently, it starts then loads. This is not ideal. \
4.) I need to export the nlp_311 environment.yml --> push to git\
5.) Update Chainlit.md \
=======
1.) Clean assignment description
2.) Each time a setting is updated the VDB is reloaded. This creates excessive wait times.
3.) The app should not start until the VDB is loaded. Currently, it starts then loads. This is not ideal.
4.) I need to export the nlp_311 environment.yml --> push to git
5.) Update Chainlit.md
>>>>>>> cfa46bba21d48ba43696fe24dad8c9fc3ad77e02

# Assignment Instructions

Exploring Vector Databases, RAG Models, and Semantic Search 


Problem: This assignment aims to tackle the issue of hallucination by LLMs by using RAG (Retrieval Augmented Generation) model to validate or reference the text generated by these tools. By combining the retriever and reader components, students develop systems capable of identifying and flagging potentially misleading or false/incorrect information generated by LLMs, thereby promoting information integrity and combating disinformation. In this assignment we will learn to build such a tool. 

Assignment Description: In this assignment, you will explore the concepts of Vectorization of Documents, Storing in Vector Databases, and retrieve related documents using RAG. You will complete three tasks to understand the process of generating embeddings for documents, storing them in a database, and performing a semantic search to retrieve the most relevant documents based on a query. 

Task 1: Generating Embeddings for Documents (30 points): In this part, you will generate embeddings for a set of documents using a pre-trained language model such as or Flan-T5. Follow these steps: 

Use this datasetLinks to an external site. for this assignment. 
Preprocess the text data (e.g., tokenization, removing stop words) if needed. 
Use a pre-trained language model (Flan-T5) to generate embeddings for each document. Split documents into multiple subcomponents if necessary. 
 

Task 2: Storing Embeddings in a Database (30 points): In this component, you will design and implement a vector database to store the generated embeddings efficiently and their corresponding texts respectively. Follow these steps: 

Choose a suitable specialized vector database (e.g., Pinecone, Milvus, FAISSLinks to an external site.). 
Design a schema to store the document embeddings along with metadata (e.g., document ID, embedding, document). 
Implement code to store the embeddings and metadata in the chosen database.  
You should create embeddings for the column context in the dataset. 
Test the database functionality by retrieving embeddings for specific documents and verifying their correctness. 
Task 3: Semantic Search with RAG Model (40 points) In this component, you will leverage the stored embeddings and implement a semantic search system using the RAG model. Follow these steps: 

Retrieve a document from the stored vector database based on a query (Sample Query: Who is the current president of USA?). 
Use an LLM to generate a response for the same query. Make the query twice. One query should be to include information only from the retrieved document (Similar to Asst. 1a) and another query should be without the retrieved document. 
Test the system by providing different queries and evaluating the accuracy and relevance of the retrieved documents and answers by measuring the BLEU, Rouge-L and BERTScore 
Create a table and compare how the outcome differs with T5 vs Flan-T5 
The dataset has four columns. Use the queries in the instruction column to retrieve the related context. 
Query over the LLM with and without the context to generate the results for the following table. 
In the table, Without RAG implies not using the context for the LLM to answer question and With RAG implies, putting the query with the retrieved context from LLM. 
The ground truth answer is the response column from the dataset. 
